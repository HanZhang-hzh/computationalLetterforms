{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text generation with Markovify\n",
    "\n",
    "By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "This notebook is a tour of how to generate text with Markov chains! Markov chains are a simple example of *predictive text generation*, a term I use to refer to methods of text generation that make use of statistical model that, given a certain stretch of text, *predicts* which bit of text should come next, based on probabilities learned from an existing corpus of text.\n",
    "\n",
    "The code is written in Python, but you don't really need to know Python in order to use the notebook. Everything's pre-written for you, so you can just execute the cells, making small changes to the code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with text files\n",
    "\n",
    "Before we get started, we'll first need some text! Grab two [plain text files from Project Gutenberg](http://www.gutenberg.org/) (or from another source of your choice) and save them to the same directory as this notebook. (I suggest working with two files because we'll be running some code explicitly to \"compare\" two texts. Also, I think seeing two different outputs from the text generation methods discussed in this notebook will help you better understand how those methods work.) The code in the following cell loads into Python variables the contents of *two plain text files*, assigned to variables `text_a` and `text_b`. You'll need to replace the filenames with the names of the files that you downloaded, keeping the quotation marks (`\"`) intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_a = open(\"Cathay.txt\").read()\n",
    "text_b = open(\"长干行.txt\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are *strings*, which are essentially just long lists of the characters that occur in the text, in the order that they occur. The code in the following cell shows the first two hundred characters of text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Song of the Bowmen of Shu ",
      " ",
      "Here we are, picking the first fern-shoots ",
      "And saying: When shall we get back to our country? ",
      "Here we are because we have the Ken-nin for our ",
      "foemen, ",
      "We have no comfort bec\n"
     ]
    }
   ],
   "source": [
    "print(text_a[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change `text_a` to `text_b` to see the output from your second text, or change `200` to a number of your choosing.\n",
    "\n",
    "The `random.sample()` function gives us a random sampling of the contents of a variable (as long as that variable is a sequence of things, like a string or a list). So, for example, to see twenty random characters from text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g',\n",
       " 'o',\n",
       " 'e',\n",
       " 'e',\n",
       " 'o',\n",
       " ' ',\n",
       " ' ',\n",
       " 'r',\n",
       " 'a',\n",
       " 'u',\n",
       " 'r',\n",
       " ' ',\n",
       " ' ',\n",
       " 'r',\n",
       " ' ',\n",
       " 'e',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " ',']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(text_a, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't incredibly helpful on its own, but you'll notice that the characters it drew (probably) more or less follow the expected letter distribution for English (i.e., lots of `e`s and `n`s and `t`s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps more interesting would be to see a randomly-sampled list of *words*. To do this, we'll make separate variables for the words in the text, using a Python function called `.split()`, which takes a string and turns it into a list of words contained in that string. The following cell makes two new variables that contain the words from both texts respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_words = text_a.split()\n",
    "b_words = text_b.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, ten random words from both text A and text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_words = []\n",
    "with open('长干行.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        words = jieba.cut(line.strip(), cut_all=True)\n",
    "        b_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['長', '干', '行', '李白', '妾', '髮', '初', '覆', '額', '，', '折', '花', '門', '前', '劇', '；', '郎', '騎', '竹', '馬', '來', '，', '遶', '床', '弄', '青梅', '。', '同居', '長', '干', '里', '，', '兩', '小', '無', '嫌', '猜', '。', '十四', '為', '君', '婦', '，', '羞', '顏', '未', '嘗', '開', '；', '低', '頭', '向', '暗', '壁', '，', '千', '喚', '不一', '一回', '，', '十五', '始', '展眉', '，', '願', '同', '塵', '與', '灰', '；', '常存', '抱柱', '信', '，', '豈', '上', '望', '夫', '臺', '？', '十六', '君', '遠', '行', '，', '瞿', '塘', '灩', '澦', '堆', '；', '五月', '不可', '觸', '，', '猿', '聲', '天上', '哀', '。', '門', '前', '遲', '行', '跡', '，', '一一', '一生', '綠', '苔', '；', '苔', '深', '不能', '掃', '，', '落', '葉', '秋', '風', '早', '。', '八月', '蝴蝶', '來', '，', '雙', '飛', '西', '園', '草', '。', '感', '此', '傷', '妾', '心', '，', '坐', '愁', '紅', '顏', '老', '。', '早晚', '下', '三', '巴', '，', '預', '將', '書', '報', '家', '；', '相迎', '不', '道', '遠', '，', '直至', '長', '風', '沙', '。']\n"
     ]
    }
   ],
   "source": [
    "print(b_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['received', 'High', 'or', 'is', 'with', 'it', 'is', '1.E.1', 'The', 'his']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(a_words, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['。', '妾', '初', '弄', '，', '早', '心', '五月', '，', '嘗']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(b_words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell uses Python's `Counter` object to count the *most common* letters in the first of these texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 5739),\n",
       " ('e', 3535),\n",
       " ('t', 2659),\n",
       " ('o', 2470),\n",
       " ('r', 2110),\n",
       " ('a', 2008),\n",
       " ('n', 2006),\n",
       " ('i', 1884),\n",
       " ('s', 1598),\n",
       " ('h', 1449),\n",
       " ('d', 1076),\n",
       " ('l', 1055)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(text_a).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the `a_words` variable gives the most frequent *words* instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 390),\n",
       " ('of', 199),\n",
       " ('to', 135),\n",
       " ('and', 135),\n",
       " ('in', 102),\n",
       " ('a', 101),\n",
       " ('with', 77),\n",
       " ('Project', 77),\n",
       " ('or', 73),\n",
       " ('And', 70),\n",
       " ('you', 70),\n",
       " ('are', 55)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(a_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the most common words in text B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 16),\n",
       " ('。', 7),\n",
       " ('；', 6),\n",
       " ('長', 3),\n",
       " ('行', 3),\n",
       " ('干', 2),\n",
       " ('妾', 2),\n",
       " ('門', 2),\n",
       " ('前', 2),\n",
       " ('來', 2),\n",
       " ('君', 2),\n",
       " ('顏', 2)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(b_words).most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov models\n",
    "\n",
    "I won't go into the precise details of how to implement a Markov chain text generator in this notebook. (I have written [a tutorial on this topic](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) elsewhere, however!) But I think it's helpful to understand the fundamentals of how Markov chain text generation works. The next question we’re going to try to answer is this: Given a stretch of text (say a string of characters, or run of words), what is most the most likely bit of text to come next?\n",
    "\n",
    "One way to answer this question is with an n-gram based Markov model. What's an n-gram? I'm glad you asked!\n",
    "\n",
    "### N-grams\n",
    "\n",
    "An n-gram is simply a sequence of units drawn from a longer sequence; in the case of text, the unit in question is usually a character or a word. For convenience, we'll call the unit of the n-gram is called its level; the length of the n-gram is called its order. For example, the following is a list of all unique character-level order-2 n-grams in the word condescendences:\n",
    "\n",
    "    co\n",
    "    on\n",
    "    nd\n",
    "    de\n",
    "    es\n",
    "    sc\n",
    "    ce\n",
    "    en\n",
    "    nc\n",
    "\n",
    "And the following is an excerpt from the list of all unique word-level order-5 n-grams in The Road Not Taken:\n",
    "\n",
    "    Two roads diverged in a\n",
    "    roads diverged in a yellow\n",
    "    diverged in a yellow wood,\n",
    "    in a yellow wood, And\n",
    "    a yellow wood, And sorry\n",
    "    yellow wood, And sorry I\n",
    "\n",
    "N-grams are used frequently in natural language processing and are a basic tool text analysis. Their applications range from programs that correct spelling to creative visualizations to compression algorithms to stylometrics to generative text.\n",
    "\n",
    "### What comes next?\n",
    "\n",
    "A Markov model for text begins with a list of n-grams. But in addition to making this list, we also keep track of what unit of text (word, character, etc.) *follows* each of those n-grams.\n",
    "\n",
    "Let’s do a quick example by hand. This is the same character-level order-2 n-gram analysis of the (very brief) text “condescendences” as above, but this time keeping track of all characters that follow each n-gram:\n",
    "\n",
    "| n-grams |\tnext? |\n",
    "| ------- | ----- |\n",
    "|co| n|\n",
    "|on| d|\n",
    "|nd| e, e|\n",
    "|de| s, n|\n",
    "|es| c, (end of text)|\n",
    "|sc| e|\n",
    "|ce| n, s|\n",
    "|en| d, c|\n",
    "|nc| e|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table, we can determine that while the n-gram `co` is followed by n 100% of the time, and while the n-gram `on` is followed by `d` 100% of the time, the n-gram `de` is followed by `s` 50% of the time, and `n` the rest of the time. Likewise, the n-gram `es` is followed by `c` 50% of the time, and followed by the end of the text the other 50% of the time.\n",
    "\n",
    "Exercise: Imagine (or even better, write out) what this table might look like if you were analyzing words instead of characters, with a source text of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains: Generating text from a Markov model\n",
    "\n",
    "The Markov models we created above don't just give us interesting statistical probabilities. It also allows us generate a *new* text with those probabilities by *chaining together predictions*. Here’s how we’ll do it, starting with the order 2 character-level Markov model of `condescendences`: (1) start with the initial n-gram (`co`)—those are the first two characters of our output. (2) Now, look at the last *n* characters of output, where *n* is the order of the n-grams in our table, and find those characters in the “n-grams” column. (3) Choose randomly among the possibilities in the corresponding “next” column, and append that letter to the output. (Sometimes, as with `co`, there’s only one possibility). (4) If you chose “end of text,” then the algorithm is over. Otherwise, repeat the process starting with (2). Here’s a record of the algorithm in action:\n",
    "\n",
    "    co\n",
    "    con\n",
    "    cond\n",
    "    conde\n",
    "    conden\n",
    "    condend\n",
    "    condendes\n",
    "    condendesc\n",
    "    condendesce\n",
    "    condendesces\n",
    "    \n",
    "As you can see, we’ve come up with a word that looks like the original word, and could even be passed off as a genuine English word (if you squint at it). From a statistical standpoint, the output of our algorithm is nearly indistinguishable from the input. This kind of algorithm—moving from one state to the next, according to a list of probabilities—is known as a Markov chain generator.\n",
    "\n",
    "### Generating with Markovify\n",
    "\n",
    "Fortunately, with the invention of digital computers, you don't have to perform this algorithm by hand! In fact, Markov chain text generation has been a pastime of poets and programmers going back [all the way to 1983](https://www.jstor.org/stable/24969024), so it should be no surprise that there are many implementations of the idea in Python that you can download and install. The one we're going to use is [Markovify](https://github.com/jsvine/markovify), a Markov chain text generation library originally developed for BuzzFeed, apparently. It comes with a lot of extra niceties that will make our lives easier, but underneath the hood, it implements an algorithm very similar to the one we just did by hand above.\n",
    "\n",
    "To install Markovify on your computer, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting markovify\n",
      "  Downloading markovify-0.9.4.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: markovify\n",
      "  Building wheel for markovify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18606 sha256=9f0b7ba72220476aa9644dd5dd3d171cdc44eac37a56bb23a161caf8add73e9d\n",
      "  Stored in directory: /Users/vanorazhang/Library/Caches/pip/wheels/76/0a/ab/8727d219981e57e6036316dd2ec2037e61ccea0c016f7ae0c1\n",
      "Successfully built markovify\n",
      "Installing collected packages: unidecode, markovify\n",
      "Successfully installed markovify-0.9.4 unidecode-1.3.6\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then run this cell to make the library available in your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import markovify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell creates a new text generator, using the text in the variable specified to build the Markov model, which is then assigned to the variable `generator_a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_b = markovify.Text(text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(generator_b.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then call the `.make_sentence()` method to generate a sentence from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Markov model on the text\n",
    "model = markovify.Text(text, state_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* You comply with the free distribution of electronic works that can be found at the wall.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.make_short_sentence()` method allows you to specify a maximum length for the generated sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.D. The copyright laws of the day we were drunk for month on month, forgetting the kings and princes.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Markovify tries to generate a sentence that is significantly different from any existing sentence in the input text. As a consequence, sometimes the `.make_sentence()` or `.make_short_sentence()` methods will return `None`, which means that in ten tries it wasn't able to generate such a sentence. You can work around this by increasing the number of times it tries to generate a sufficiently unique sentence using the `tries` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have no comfort.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, tries=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by disabling the check altogether with `test_output=False` (note that this means the generator will occasionally return stretches of text that are present in the source text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Mei Sheng.\n"
     ]
    }
   ],
   "source": [
    "print(generator_a.make_short_sentence(40, test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the order\n",
    "\n",
    "When you create the model, you can specify the order of the model using the `state_size` parameter. It defaults to 2. Let's make two model with different orders and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_1 = markovify.Text(text_a, state_size=1)\n",
    "gen_a_4 = markovify.Text(text_a, state_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order 1\n",
      "No longer the same way for explanation, and with the efforts and princes.\n",
      "\n",
      "order 4\n",
      "In another I find a perfect speech in a literality which will be to many most unacceptable.\n"
     ]
    }
   ],
   "source": [
    "print(\"order 1\")\n",
    "print(gen_a_1.make_sentence(test_output=False))\n",
    "print()\n",
    "print(\"order 4\")\n",
    "print(gen_a_4.make_sentence(test_output=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the higher the order, the more the sentences will seem \"coherent\" (i.e., more closely resembling the source text). Lower order models will produce more variation. Deciding on the order is usually a matter of taste and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the level\n",
    "\n",
    "Markovify, by default, works with *words* as the individual unit. It doesn't come out-of-the-box with support for character-level models. The following code defines a new kind of Markovify generator that implements character-level models. Execute it before continuing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any of the parameters you passed to `markovify.Text` you can also pass to `SentencesByChar`. The `state_size` parameter still controls the order of the model, but now the n-grams are characters, not words.\n",
    "\n",
    "The following cell implements a character-level Markov text generator for the word \"condescendences\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "con_model = SentencesByChar(\"condescendences\", state_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the output—it'll be a lot like what we implemented by hand earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'condendescencencendes'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con_model.make_sentence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can use a character-level model on any text of your choice. So, for example, the following cell creates a character-level order-7 Markov chain text generator from text A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_a_char = SentencesByChar(text_a, state_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "長干行\n",
      "李白\n",
      "妾髮初覆額，\n",
      "折花門前劇；\n",
      "郎騎竹馬來，\n",
      "遶床弄青梅。\n",
      "同居長干里，\n",
      "兩小無嫌猜。\n",
      "十四為君婦，\n",
      "羞顏未嘗開；\n",
      "低頭向暗壁，\n",
      "千喚不一回，\n",
      "十五始展眉，\n",
      "願同塵與灰；\n",
      "常存抱柱信，\n",
      "豈上望夫臺？\n",
      "十六君遠行，\n",
      "瞿塘灩澦堆；\n",
      "五月不可觸，\n",
      "猿聲天上哀。\n",
      "門前遲行跡，\n",
      "一一生綠苔；\n",
      "苔深不能掃，\n",
      "落葉秋風早。\n",
      "八月蝴蝶來，\n",
      "雙飛西園草。\n",
      "感此傷妾心，\n",
      "坐愁紅顏老。\n",
      "早晚下三巴，\n",
      "預將書報家；\n",
      "相迎不道遠，\n",
      "直至長風沙。\n"
     ]
    }
   ],
   "source": [
    "print(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below prints out a random sentence from this generator. (The `.replace()` is to get rid of any newline characters in the output.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bosque taketh blossom?\n"
     ]
    }
   ],
   "source": [
    "print(gen_a_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_b_char = SentencesByChar(text_b, state_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "長風沙。 十六君婦， 感此傷妾心， 預將書報家； 低頭向暗壁， 低頭向暗壁， 直至長干里， 十六君婦， 雙飛西園草。 坐愁紅顏老。 同塵與灰； 願同居長干行 一生綠苔； 八月不一生綠苔； 猿聲天上望夫臺？ 羞顏未嘗開； 感此傷妾髮初覆額， 一一回， 遶床弄青梅。 低頭向暗壁， 低頭向暗壁， 同塵與灰； 門前遲行， 十四為君遠， 兩小無嫌猜。 瞿塘灩澦堆； 感此傷妾心， 早。 直至長干行 十六君遠行 李白 郎騎竹馬來， 千喚不能掃， 同塵與灰； 同居長干里， 折花門前遲行， 兩小無嫌猜。 苔； 直至長干行 相迎不道遠， 折花門前劇； 猿聲天上哀。 一回， 早晚下三巴， 苔； 直至長干里， 坐愁紅顏未嘗開； 十四為君婦， 郎騎竹馬來， 妾髮初覆額， 苔； 八月不一生綠苔深不一回， 預將書報家； 郎騎竹馬來， 相迎不能掃， 八月不能掃， 十六君遠， 郎騎竹馬來， 預將書報家； 常存抱柱信， 願同居長風早晚下三巴， 五月蝴蝶來， 願同居長風沙。\n"
     ]
    }
   ],
   "source": [
    "print(gen_b_char.make_sentence(test_output=False).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining models\n",
    "\n",
    "Markovify has a handy feature that allows you to *combine* models, creating a new model that draws on probabilities from both of the source models. You can use this to create hybrid output that mixes the style and content of two (or more!) different source texts. To do this, you need to create the models independently, and then call `.combine()` to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_a = markovify.Text(text_a)\n",
    "generator_b = markovify.Text(text_b)\n",
    "combo = markovify.combine([generator_a, generator_b], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<markovify.text.Text object at 0x1069a3790>\n"
     ]
    }
   ],
   "source": [
    "print(generator_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bit of code `[0.5, 0.5]` controls the \"weights\" of the models, i.e., how much to emphasize the probabilities of any model. You can change this to suit your tastes. (E.g., if you want mostly text A with but a *soupçon* of text B, you would write `[0.9, 0.1]`. Try it!) \n",
    "\n",
    "Then you can create sentences using the combined model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And within, the mistress, in the same way over the portals of Sei-go-yo, And clings to the north of Raku-hoku, Till we had nothing but thoughts and memories in common.\n"
     ]
    }
   ],
   "source": [
    "print(combo.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together\n",
    "\n",
    "I've pre-written some code below to make it easy for you to experiment and produce output from Markovify. Just make adjustments to the values assigned to the variables in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change to \"word\" for a word-level model\n",
    "level = \"char\"\n",
    "# controls the length of the n-gram\n",
    "order = 7\n",
    "order2 = 1\n",
    "# controls the number of lines to output\n",
    "output_n = 14\n",
    "# weights between the models; text A first, text B second.\n",
    "# if you want to completely exclude one model, set its corresponding value to 0\n",
    "weights = [0.5, 0.5]\n",
    "# limit sentence output to this number of characters\n",
    "length_limit = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesByChar(markovify.Text):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Rihaku FOUR POEMS OF DEPARTURE Light rain is on the going, And on the feathery banners.\n",
      "With yellow gold and white jewels, we paid for songs and laughter And we went on living in the heart.\n",
      "None\n",
      "Sorrow to go, and sorrow, sorrow like rain.\n",
      "With yellow gold and white jewels, we paid for songs and laughter And we guardsmen fed to the north of the hill at his horse's bridle.\n",
      "The sea's colour moves at the wing-flapping storks, He returns by way of the Bowmen of Shu Here we are because we have the Ken-nin for our foemen, We have no rest, three battles a month.\n",
      "Moaneth alway my mind's lust That I fare forth, that I on high streams The salt-wavy tumult traverse alone.\n",
      "Lament of the Frontier Guard By the south side of the castle, To the dance of the turning and twisting waters, Into a valley of the whole book of translations.\n",
      "Five clouds hang aloft, bright on the going, And on the stone-cliffs beaten, fell on the water.\n",
      "In another I find a perfect speech in a man's tide go, turn it to twain.\n",
      "By heaven, his horses even, are tired.\n",
      "And I will come out to Hori, to look at the wall.\n",
      "The generals are on the going, And on the paved way of the turning and twisting waters, Into a valley of the Bowmen of Shu Here we must be careful.\n",
      "Sunset like the flowers falling at Spring's end Confused, whirled in a man's face, Clouds grow out of habit.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Read the input text file\n",
    "with open('Cathay.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the Markov chain model\n",
    "model = markovify.Text(text, state_size=2)\n",
    "\n",
    "# Generate a new poem\n",
    "for i in range(15):\n",
    "    poem = model.make_sentence()\n",
    "    print(poem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The lines beginning with `#` are \"comments\"—they don't do anything, they're just there to explain what's happening in the code.)\n",
    "\n",
    "After making your changes above, run the cell below to generate text according to your parameters. Repeat as necessary until you get something you really like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By Kutsugen.\n",
      "\n",
      "Horses, his horses are on the base of old hills.\n",
      "\n",
      "Haughty their buried bodies ",
      "Be an unlikely treasure lasting, with the dragon-scales, going grass in the middle kingdom, ",
      "Three hundred and sixty thousand from the sea and frosts, ",
      "High heaps, covered with the wave's slash, ",
      "Yet longing comes to an end.\n",
      "\n",
      "Also she has no excuse on account of weather.\n",
      "\n",
      "Ah, how shall you know the whole book of translations.\n",
      "\n",
      "The leaves fall early this autumn.\n",
      "\n",
      "The eastern wind brings the green on this flute, ",
      "Their voice is in these unquestionable poems.\n",
      "\n",
      "And your feet were by frost benumbed.\n",
      "\n",
      "Taking Leave of a Friend ",
      " ",
      " ",
      "Blue mountains ",
      "white-headed.\n",
      "\n",
      "Cuckoo calleth with gloomy crying lone-flyer, ",
      "Whets for the wall.\n",
      "\n",
      "The City of Chokan: ",
      "Two small people, with courtezans, going greener, ",
      "But you, Sir, had better take wine ere your departing of old acquaintances ",
      "Who bow over the river Kiang ",
      " ",
      " ",
      "Ko-jin goes west from Ko-kaku-ro, ",
      "The smoke-flowers.\n",
      "\n",
      "Coldly afflicted, ",
      "My feet when you went north to San palace, ",
      "And if you are coming down through a thousand valleys full of voices and ",
      "pine-winds.\n",
      "\n",
      "I call in the boy, ",
      "Have him sit on his knees here ",
      "To seal this, ",
      "And your feet were by frost benumbed.\n",
      "\n",
      "King So's terrace.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "for i in range(output_n):\n",
    "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating with non-prose text\n",
    "\n",
    "Markovify assumes you're feeding it prose, i.e., a text file that can be parsed into sentences by separating on sentence-ending punctuation. But often you're *not* working with text like this. For example, let's generate some sonnets. First, download [this plaintext version of Shakespeare's sonnets](https://raw.githubusercontent.com/aparrish/plaintext-example-files/master/sonnets.txt) and keep it in the same directory as this notebook. We'll define the sonnet-generating task as consisting of (a) training a Markov chain on lines of poetry and then (b) generating a sequence of fourteen lines of poetry. Since the *line* is the unit now and not the *sentence*, we need to use Markovify's `NewlineText` class instead of `Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_text = open(\"Cathay.txt\").read()\n",
    "sonnets_model = markovify.NewlineText(sonnets_text, state_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can generate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I stopped scowling, I desired my forehead I played about the North Gate, the gate now, the heart, And you departed, You went out. By the narrows of Ernest Fenollosa's notes by a thousand frosts, High heaps, covered with trees and on the beginning of swirling eddies, And I never looked at the eastward-flowing waters. Petals are given over the back-swirling eddies, But to-days men for the dawn And she puts forth a courtezan in a certain poem :\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnets_model.make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "\n",
    "# Read the text file\n",
    "with open('长干行.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split the text into individual characters\n",
    "chars = list(text)\n",
    "\n",
    "# Train a Markov model on the characters\n",
    "model = markovify.Text(chars, state_size=2)\n",
    "\n",
    "# Generate a new poem by combining characters\n",
    "new_poem = \"\"\n",
    "\n",
    "# Keep generating new lines until we have 4 lines\n",
    "while len(new_poem.split('\\n')) < 4:\n",
    "    line = model.make_sentence()\n",
    "    if line is not None:\n",
    "        new_poem += line + '\\n'\n",
    "\n",
    "print(new_poem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now make a sonnet, sorta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note.—Jewel stairs, but has come to cut the mad chase through the bridge-rail. The sea's colour moves at the eastward-flowing waters. Petals are they hang over to watch out to the North Gate, With head-gear glittering against the Bridge at the middle kingdom, Three hundred and sixty thousand, And evening drives them away! The Beautiful Toilet Blue, blue plums. And no excuse on the eastward-flowing waters. Petals are already yellow with head-trappings of Ernest Fenollosa's notes by a thousand gates, At morning there is the door. Slender, she puts forth a Letter While my head, I have been gone waters and towers to many most unacceptable. The sea's colour moves at the narrows of a thousand autumns, Unwearying autumns. For them away! The leaves fall early this autumn, A gracious spring, turned to the West garden, They ride upon dragon-like horses, Upon horses with his mistress, With her too much alone.\n",
      "I stopped scowling, I never looked back. At sixteen you departed, You came by the gate now, the wind blows full of wars-men, spread over the streets make sorrowful noise overhead. You walked about the wall. Called to, a courtezan in the old days, Though they think it will be to one line out to many most unacceptable. The leaves her too much alone.\n",
      "I.e., we went out. By Rihaku.\n",
      "By the mad chase through the yellow with trees and with his mistress, in the mad chase through the gone waters and with a thousand gates, At fifteen I looked back. At sixteen you have not come early, for offence and with his mistress, in the narrows of Chokan: Two small people, without dislike or suspicion. At fifteen I climb the going, And you are already yellow with drums and forever. Why should I climb the close garden. And they go into far Ku-to-Yen, by on each border.\n",
      "I climb the portals of the streets make sorrowful noise overhead. You came by the bridge head, I have overfilled the perfumed air and grass; Who brought the flaming imperial anger? Who has no children of yellow-metal, And sorrow, sorrow at the wall. Called to, a long way, nor is grown, the grass in the old days, And the empire to the grass in the same way for their passage. Haughty their passing, Haughty their passage. Haughty their passing, Haughty their passing, Haughty their steps as they compared to add to be mingled with blue plums. And they compared to blood-ravenous autumn, A gracious spring, turned to the cloud and he his own skiffs-man!\n",
      "Lament of her stockings. The poem is especially prized because she puts forth a thousand times, I climb the gardens. Night and towers to go, and the front gate, pulling flowers. You walked about the beginning of weather. Also she has not come to meet you, As far borders. They ride upon them, No longer the eastward-flowing waters. Petals are coming down through the river of time until now! Trees fall, the different mosses, Too deep to many most unacceptable. The River-Merchant's Wife: a thousand autumns, Unwearying autumns. For them on each border.\n",
      "I.e., we have been warring from the stairs, therefore a Letter While my forehead I climb the West garden, They hurt me, I have not come to add to meet you, As far Ku-to-Yen, by the gate now, the barbarous land: Desolate castle, the old days, Though they think it will come to the village of warfare upon dragon-like horses, Upon horses with his own skiffs-man!\n",
      "The leaves her hair was cause of wars-men, spread over a thousand autumns, Unwearying autumns. For them is especially prized because she has brought this autumn, A turmoil of time until now! Trees fall, the gate now, the court, and sorrow, sorrow like rain. Sorrow to add to cease from translation. True, I looked at the dreary sorrow returning, Desolate, desolate fields, And evening drives them is something to one end of face, hesitates, passing the river of the old days, Though they hang over the wide desert. There is especially prized because she has come early, for the old days, And she was a sot, Who among them the grass in the grass about the men for offence and day are flowers to pass? Who has no children of wars-men, spread over the river Kiang, Please let me know beforehand, And on each border.\n",
      "I stopped scowling, I never looked at Ten-Shin March has come to clear flutes and curious food, To the Frontier Guard By Rihaku.\n",
      "I looked back. At fifteen I grow older, If you know beforehand, And what are they think it will come to the Frontier Guard By Rihaku.\n",
      "By the bridge-rail. The sea's colour moves at the Bridge at the front gate, pulling flowers. You came by the narrows of hate! Who now east, now goes yellow with August Over the cloud and girls dancing, To the West garden, They ride upon them, No longer the willows have been warring from one end of Ernest Fenollosa's notes by on account of the same way for their steps as they think it entirely perplexity that causes me to cut the end of the men are flowers to blood-ravenous autumn, in the stairs, but has married a perfect speech in the river Kiang, Please let me know the men are on the midmost of her youth, White, white with his own skiffs-man!\n",
      "I find little to pass? Who has brought the stairs, but has come to the bridge-rail. The lords go into far as Cho-fu-Sa.\n",
      "By the seventy couples; To the Bridge at Ten-Shin March has brought this to the narrows of the walls and forever, and the cloud and with his mistress, in the back-swirling eddies, But to-days men of weather. Also she has married a sot, Who has not the bridge head, Peach boughs hang in vain, And no wall left to cut straight across my hair unbound, and clear singing; To the wide desert. There is as they hang in the midmost of Chokan: Two small people, without dislike or suspicion. At sixteen you went on each border.\n",
      "The River-Merchant's Wife: a long way, nor is it entirely perplexity that causes me know beforehand, And they think it entirely perplexity that causes me know the Frontier Guard By the throne, And she has come out And what are flowers to the sky, the gate now, the grass goes drunkenly out And the old days, And no direct reproach.\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this with a character-level model is a bit more tricky. I've written code in the cell below that defines a new class, `LinesByCharacter` that works like `NewlineText` but operates character-by-character instead of word-by-word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesByChar(markovify.NewlineText):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a character model with the sonnets, line by line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets_char_model = LinesByChar(sonnets_text, state_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And generate new sonnets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "In anotherefore alread overhead-trappings. ",
      "And with Rihaku.\n",
      "None\n",
      "None\n",
      "The looked her hang over laught the men are a perfumed about this to blows:\n",
      "None\n",
      "None\n",
      "By Riokushu, ",
      "They hurt merely pring autumn, the sea's come to the poem :\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    print(sonnets_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New moods\n",
    "\n",
    "Character-level Markov chains are especially suitable, in my experience, for generating shorter texts, like individual words or names. Let's generate names of new moods using this technique. First, download [this JSON file of moods](https://raw.githubusercontent.com/dariusk/corpora/9bb62927951f79bec2454f29d71b6e9b28d874b1/data/humans/moods.json) from [Corpora Project](https://github.com/dariusk/corpora/) and save to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the JSON file and grab just the list of words naming moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './moods.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mood_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./moods.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m      3\u001b[0m moods \u001b[38;5;241m=\u001b[39m mood_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoods\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './moods.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "mood_data = json.loads(open(\"./moods.json\").read())\n",
    "moods = mood_data['moods']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to use this is to make one big string with the moods joined together with newlines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_text = \"\\n\".join(moods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use `LinesByChar` to make the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods_char_model = LinesByChar(moods_text, state_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila, new moods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cowarlessive\n",
      "toughted\n",
      "lyricapative\n",
      "convictorted\n",
      "embarratived\n",
      "teasane\n",
      "feistative\n",
      "powerloaded\n",
      "bothetic\n",
      "rejuvenaccepted\n",
      "teassioned\n",
      "innovatired\n",
      "chieved\n",
      "alievoless\n",
      "incomperational\n",
      "greterpressived\n",
      "imped\n",
      "soreborn\n",
      "lethant\n",
      "jadequashamed\n",
      "intemplifeless\n",
      "deterrientalkatirical\n",
      "enconfidead\n",
      "strappallous\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    print(moods_char_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chars\n",
    "#chinese chars parsing and markov chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_model(model, n, seq):\n",
    "    # make a copy of seq and append None to the end\n",
    "    seq = list(seq[:]) + [None]\n",
    "    for i in range(len(seq)-n):\n",
    "        # tuple because we're using it as a dict key!\n",
    "        gram = tuple(seq[i:i+n])\n",
    "        next_item = seq[i+n]            \n",
    "        if gram not in model:\n",
    "            model[gram] = []\n",
    "        model[gram].append(next_item)\n",
    "\n",
    "def markov_model(n, seq):\n",
    "    model = {}\n",
    "    add_to_model(model, n, seq)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def gen_from_model(n, model, start=None, max_gen=100):\n",
    "    if start is None:\n",
    "        start = random.choice(list(model.keys()))\n",
    "    output = list(start)\n",
    "    for i in range(max_gen):\n",
    "        start = tuple(output[-n:])\n",
    "        next_item = random.choice(model[start])\n",
    "        if next_item is None:\n",
    "            break\n",
    "        else:\n",
    "            output.append(next_item)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_model_from_sequences(n, sequences):\n",
    "    model = {}\n",
    "    for item in sequences:\n",
    "        add_to_model(model, n, item)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_generate_from_sequences(n, sequences, count, max_gen=100):\n",
    "    starts = [item[:n] for item in sequences if len(item) >= n]\n",
    "    model = markov_model_from_sequences(n, sequences)\n",
    "    return [gen_from_model(n, model, random.choice(starts), max_gen)\n",
    "           for i in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "忧心之矣，\n",
      "室家之士，\n",
      "攸彼南山，\n",
      "曰归曰归，\n",
      "亦余心之忧。\n",
      "纤纤出素手。\n",
      "常存抱柱信，\n",
      "坐愁紅顏老。\n",
      "采薇采薇，\n",
      "羞顏未嘗開；\n",
      "宁赏宁处。\n",
      "靡室靡家，\n",
      "纤纤出素手。\n",
      "苔深不能掃，\n",
      "盈盈楼上女，\n",
      "豈上望夫臺？\n",
      "猿聲天上哀。\n",
      "苔深不能掃，\n",
      "靡室靡家，\n"
     ]
    }
   ],
   "source": [
    "frost_lines = [line.strip() for line in open(\"长干行.txt\").readlines()]\n",
    "for item in markov_generate_from_sequences(5, frost_lines, 19):\n",
    "    text_b = ''.join(item)\n",
    "    print(text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"character\" else SentencesByChar\n",
    "gen_b = model_cls(text_b, state_size=2)\n",
    "generated_text = gen_b.make_sentence()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating poems for the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesByChar(markovify.NewlineText):\n",
    "    def word_split(self, sentence):\n",
    "        return list(sentence)\n",
    "    def word_join(self, words):\n",
    "        return \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "折花門前遲行跡\n",
      "同居長干行\n",
      "今为荡子行不归\n",
      "今为荡子行不归\n",
      "同居長干行\n",
      "同居長干行\n",
      "折花門前遲行跡\n",
      "折花門前遲行跡\n",
      "同居長干行\n",
      "折花門前遲行跡\n",
      "今为荡子行不归\n",
      "折花門前遲行跡\n",
      "今为荡子行不归\n",
      "今为荡子行不归\n",
      "折花門前遲行跡\n",
      "折花門前遲行跡\n",
      "同居長干行\n",
      "今为荡子行不归\n",
      "今为荡子行不归\n"
     ]
    }
   ],
   "source": [
    "# Load the source text\n",
    "with open(\"yao.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "#model = markovify.Text(text, state_size=1)\n",
    "model = LinesByChar(text)\n",
    "\n",
    "# Generate five poems\n",
    "for i in range(19):\n",
    "    poem = model.make_sentence(tries=1000)\n",
    "    print(poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovifyInterface:\n",
    "    def __init__(self):\n",
    "        self.models = []\n",
    "        self.texts = []\n",
    "        self.text_cnt = 0\n",
    "        self.combined_model: Union[None, Text] = None\n",
    "\n",
    "    def add_text(self, texts, split_by=\"\\n\"):\n",
    "        for t in texts.split(split_by):\n",
    "            if len(t) > 0:\n",
    "                self.texts.append(t)\n",
    "\n",
    "    def gen_model(self):\n",
    "        self.combined_model = markovify.combine(models=self.models)\n",
    "\n",
    "    def convent_text_2_model(self):\n",
    "        while len(self.texts) > 0:\n",
    "            text = self.texts.pop()\n",
    "            self.models.append(markovify.Text(text))\n",
    "\n",
    "    def make_sentence(self, max_times_invocation=10):\n",
    "        self.convent_text_2_model()\n",
    "        self.gen_model()\n",
    "        sentence = None\n",
    "        index = 0\n",
    "        while sentence is None and index < max_times_invocation:\n",
    "            sentence = self.combined_model.make_sentence()\n",
    "            index += 1\n",
    "        return sentence\n",
    "\n",
    "    def to_json(self):\n",
    "        self.convent_text_2_model()\n",
    "        self.gen_model()\n",
    "        return self.combined_model.to_json()\n",
    "\n",
    "    def load_json(self, json_str):\n",
    "        self.combined_model = markovify.Text.from_json(json_str)\n",
    "\n",
    "# unit test\n",
    "inter = MarkovifyInterface()\n",
    "\n",
    "#\n",
    "inter.add_text(\"耀\\n耀\", split_by=\"\\n\")\n",
    "inter.make_sentence()\n",
    "\n",
    "#\n",
    "json_str = inter.to_json()\n",
    "inter.load_json(json_str)\n",
    "assert json_str == inter.to_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pronouncing\n",
      "  Downloading pronouncing-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmudict>=0.4.0\n",
      "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources<6.0.0,>=5.10.1\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/vanorazhang/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata<6.0.0,>=5.1.0->cmudict>=0.4.0->pronouncing) (3.8.0)\n",
      "Building wheels for collected packages: pronouncing\n",
      "  Building wheel for pronouncing (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6236 sha256=9b1d5f0befabb60ad909482b3f02d276f7442134a63ed1777d97c0e6bc15af26\n",
      "  Stored in directory: /Users/vanorazhang/Library/Caches/pip/wheels/ee/d4/c2/fb8c0e2009b75358874506ff2ce1ee79370b6ef5cf08922206\n",
      "Successfully built pronouncing\n",
      "Installing collected packages: importlib-resources, importlib-metadata, cmudict, pronouncing\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.12.0 pronouncing-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m poem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m poem:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Generate a sentence that ends with a rhyming word\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sentence:\n\u001b[1;32m     22\u001b[0m         words \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39msplit()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/markovify/text.py:237\u001b[0m, in \u001b[0;36mText.make_sentence\u001b[0;34m(self, init_state, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# pragma: no cover # see coveragepy/issues/198\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_output \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrejoined_text\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_sentence_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmot\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_join(words)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/markovify/text.py:189\u001b[0m, in \u001b[0;36mText.test_sentence_output\u001b[0;34m(self, words, max_overlap_ratio, max_overlap_total)\u001b[0m\n\u001b[1;32m    187\u001b[0m gram_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((\u001b[38;5;28mlen\u001b[39m(words) \u001b[38;5;241m-\u001b[39m overlap_max), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    188\u001b[0m grams \u001b[38;5;241m=\u001b[39m [words[i : i \u001b[38;5;241m+\u001b[39m overlap_over] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(gram_count)]\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m grams:\n\u001b[1;32m    190\u001b[0m     gram_joined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_join(g)\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gram_joined \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrejoined_text:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "import pronouncing\n",
    "\n",
    "class RhymingText(markovify.Text):\n",
    "    def word_rhyme(self, word):\n",
    "        return pronouncing.rhymes(word)\n",
    "\n",
    "# Load the source text\n",
    "with open(\"Cathay.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Train the model\n",
    "model = RhymingText(text)\n",
    "\n",
    "# Generate five poems with rhymes\n",
    "for i in range(5):\n",
    "    poem = None\n",
    "    while not poem:\n",
    "        # Generate a sentence that ends with a rhyming word\n",
    "        sentence = model.make_sentence(tries=100)\n",
    "        if sentence:\n",
    "            words = sentence.split()\n",
    "            rhyming_word = words[-1]\n",
    "            rhyming_candidates = model.word_rhyme(rhyming_word)\n",
    "            for candidate in rhyming_candidates:\n",
    "                if candidate in model.chain.model:\n",
    "                    poem = sentence + \" \" + candidate\n",
    "                    break\n",
    "    \n",
    "    # Format the poem with multiple lines\n",
    "    lines = poem.split()\n",
    "    poem_lines = []\n",
    "    current_line = \"\"\n",
    "    for word in lines:\n",
    "        if len(current_line + \" \" + word) > 20:  # change 20 to the desired line length\n",
    "            poem_lines.append(current_line.strip())\n",
    "            current_line = \"\"\n",
    "        current_line += word + \" \"\n",
    "    poem_lines.append(current_line.strip())\n",
    "    \n",
    "    # Print the formatted poem\n",
    "    print(\"\\n\".join(poem_lines))\n",
    "    print()  # add an extra newline between poems\n",
    "print(poem_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Define the path of the input and output files\n",
    "input_file_path = 'yao.txt'\n",
    "output_file_path = '耀.txt'\n",
    "\n",
    "# Define a list of Chinese punctuation marks to be removed\n",
    "chinese_punctuation = '。，！？；：‘’“”【】（）《》'\n",
    "\n",
    "# Open the input file for reading\n",
    "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "    # Read the contents of the input file and remove line breaks\n",
    "    text = input_file.read().replace('\\n', '')\n",
    "    # Remove all Chinese punctuation marks from the text\n",
    "    text = ''.join([char for char in text if char not in chinese_punctuation])\n",
    "    # Open the output file for writing\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        # Write the modified text to the output file\n",
    "        output_file.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "曰 归 曰 归 曰 归\n",
      "曰 归 曰 归 曰 归\n",
      "None\n",
      "曰 归 曰 归 曰 归 曰 归 曰 归\n",
      "None\n",
      "曰 归 曰 归 曰 归\n",
      "None\n",
      "曰 归 曰 归 曰 归 曰 归\n",
      "None\n",
      "None\n",
      "曰 归 曰 归 曰 归 曰 归\n",
      "曰 归 曰 归 曰 归 曰 归\n",
      "None\n",
      "None\n",
      "曰 归 曰 归 曰 归 曰 归\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "import jieba\n",
    "#from model_gen import MarkovifyInterface\n",
    "from typing import Union\n",
    "from markovify import Text\n",
    "jieba.setLogLevel(\"WARNING\")\n",
    "\n",
    "def get_1():\n",
    "    # Get raw text as string.\n",
    "    with open(\"yao.txt\") as f:\n",
    "        text = f.read()\n",
    "    text = \" \".join(list(jieba.lcut(text)))\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_2():\n",
    "    # Get raw text as string.\n",
    "    with open(\"yao.txt\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "inter = MarkovifyInterface()\n",
    "inter.add_text(get_1(), split_by=\"\\n\")\n",
    "inter.add_text(get_2(), split_by=\"\\n\")\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(15):\n",
    "    print(inter.make_sentence()) #.replace(\" \", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "采薇采薇采薇薇亦作止曰归曰归岁亦莫止靡室靡家玁狁之故不如娶女留傅自牧采薇采薇薇亦柔止曰归曰归心亦忧止忧心之矣亦余心之忧室家之士攸彼南山淑人君子宁赏宁处青青河畔草青青河畔草郁郁园中柳盈盈楼上女皎皎当窗牖娥娥红粉妆纤纤出素手昔为倡家女今为荡子妇荡子行不归空床难独守長干行李白妾髮初覆額折花門前劇郎騎竹馬來遶床弄青梅同居長干里兩小無嫌猜十四為君婦羞顏未嘗開低頭向暗壁千喚不一回十五始展眉願同塵與灰常存抱柱信豈上望夫臺十六君遠行瞿塘灩澦堆五月不可觸猿聲天上哀門前遲行跡一一生綠苔苔深不能掃落葉秋風早八月蝴蝶來雙飛西園草感此傷妾心坐愁紅顏老早晚下三巴預將書報家相迎不道遠直至長風沙\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "import jieba\n",
    "#from model_gen import MarkovifyInterface\n",
    "from typing import Union\n",
    "from markovify import Text\n",
    "jieba.setLogLevel(\"WARNING\")\n",
    "\n",
    "def get_1():\n",
    "    # Get raw text as string.\n",
    "    with open(\"yao.txt\") as f:\n",
    "        text = f.read()\n",
    "    text = \" \".join(list(jieba.lcut(text)))\n",
    "    return text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = markovify.Text(text, state_size=2)\n",
    "\n",
    "# Generate five poems\n",
    "for i in range(10):\n",
    "    poem = model.make_sentence(tries=10)\n",
    "    print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'level' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_cls \u001b[38;5;241m=\u001b[39m markovify\u001b[38;5;241m.\u001b[39mText \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlevel\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m SentencesByChar\n\u001b[1;32m      2\u001b[0m gen_a \u001b[38;5;241m=\u001b[39m model_cls(text_a, state_size\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m      3\u001b[0m gen_b \u001b[38;5;241m=\u001b[39m model_cls(text_b, state_size\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'level' is not defined"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "for i in range(output_n):\n",
    "    out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "    out = out.replace(\"\\n\", \" \")\n",
    "    print(out)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'level' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_cls \u001b[38;5;241m=\u001b[39m markovify\u001b[38;5;241m.\u001b[39mText \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlevel\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m SentencesByChar\n\u001b[1;32m      2\u001b[0m gen_a \u001b[38;5;241m=\u001b[39m model_cls(text_a, state_size\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m      3\u001b[0m gen_b \u001b[38;5;241m=\u001b[39m model_cls(text_b, state_size\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'level' is not defined"
     ]
    }
   ],
   "source": [
    "model_cls = markovify.Text if level == \"word\" else SentencesByChar\n",
    "gen_a = model_cls(text_a, state_size=order)\n",
    "gen_b = model_cls(text_b, state_size=order)\n",
    "gen_combo = markovify.combine([gen_a, gen_b], weights)\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(output_n):\n",
    "        out = gen_combo.make_short_sentence(length_limit, test_output=False)\n",
    "        out = out.replace(\"\\n\", \" \")\n",
    "        f.write(out)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "干里 忧 ， ， 李白 ， A.D. 河畔 ？ 願同塵與灰； has Letter blood-ravenous Are 。 He 女 曰 Also month. rock, a a bundles century 五月不可觸， 青青 the account wandering Called Toilet now across old ； ， A.D. 归 雙飛 phoenix ， clouds has October? 青青 折花門 ； 折花門前劇； and of Shin 之 不可 the the green 楼上 归 Rakuyo, to 采薇 river 心亦忧止。 Kiang, 哀 yellow beforehand, turmoil. 騎 。 堆 of 落葉秋風早。 itself 之 青梅 The of 采薇采薇， to 娥 this tumult 瞿塘灩澦堆； 早晚下三巴， be cloud 暗壁 竹馬來 余心 盈盈楼上女， body. village. 忧心 信 weather. 。 To over 猜 gone, 行不归 落葉秋風 玁 心 。 。 臺 cut There no 青青河畔草 you 薇 his autumn, still with August 長干行 纤纤出素手。 to 玁狁之故。 eyebrows like 。 前劇 薇 紅顏老 of 願同塵 The 当 thousand, City Flowers 空床 A 粉妆 淑人君子 遲行跡 彼 荡子 相迎 草 長 抱柱 the 床 on The cut 自牧 from 感此 室家之士， 采薇 ； high and 瞿塘灩 the 采薇 直至長風沙。 手 郎騎竹馬來， 。 Choan 早晚 Gen. left ， 窗牖 亦 a bridle. 园中 澦 I 妇 to 空床难独守。 豈上 played 今为荡子妇。 how straight 郁郁园中柳。 。 八月 矣 未嘗開 狁 門前 十六君遠行， 來 ， 青青河畔草， a 豈上望夫臺？ And 不能 rest, on rain. 红 ， 独守 the of 淑人君子， know 薇亦作止。 昔为 ， wind them thousand 室靡家 shall my ； 故 預將書 ， Till 难 ancient And let 纤纤 十六 ， 感此傷妾心， 。 a River 低頭 I 同居 hear 妾髮初覆額， moon. 留傅自牧。 doom-gripped 。 the 。 over 十五始展眉， 綠苔 a lords is the 曰 ， 荡子行不归， sun 不如娶女， 娶 曰 。 。 岁亦莫止。 昔为倡家女， portals I 薇亦柔止。 觸 eyebrows 皎 Raku-hoku, regret hang 一回 宁赏 summerward, South salt-wavy no Ah, excuse streams And Century 。 Over three 心亦忧止 the traverse ， to barbarian 岁 of 天上 The gates, 弄 女 ， 郎 I 草 采薇 door. Blue, ， bodeth And thousand, ， know in 同居長干里， Chancellor 采薇 娥 at 作止 猿聲 is 柔止 出素 ， The 蝴蝶 turmoil. the 一生 娥娥红粉妆， crying, 荡子 長 預將書報家； that 八月蝴蝶來， bridge-head. forth the 皎皎当窗牖。 干行 send 不道 shall 曰归曰归， go whitened ， ， 展眉 猿聲天上哀。 we the When bluish it 長 遶床弄青梅。 皎 ， And ， 風沙 千喚 亦 。 the turned friend, 常存抱柱信， willows sorrow 始 。 遠 painted— The 攸 采薇采薇， 室家 曰 采薇 李白 十五 no 攸彼南山， Now me 靡室靡家， forehead wall 宁赏宁处。 Sei Please let of back 坐愁 she ， 早 saying: 十四為君婦， 靡 fern-shoots ， painted 望夫 郁郁 五月 坐愁紅顏老。 相迎不道遠， 一 have 河畔 north 柳 亦 西園 horse's ， Beautiful 忧心之矣， 妾 And 莫止 spring, 8th 直至 之士 下三巴 River-Merchant's 今为 門前遲行跡， you blue 。 The 不如 。 ； 羞顏未嘗開； 與 留傅 雙飛西園草。 of ， go 十四 兩小無嫌 caps 归 千喚不一回， thousand 傷妾 was 不 女 Five 一一生綠苔； 君遠行 倡家 報家 南山 。 Wife: Desert 。 merely on a 常存 So-Kin 草 to about into 向 she 亦 Century 曰归曰归， 之 羞顏 to, 苔深 of 遶 alone. battles B.C. ， 4th 盈盈 green 苔深不能掃， 兩小無嫌猜。 alone. ， singeth not 髮 ， 低頭向暗壁， 亦余心之忧。 are flows ， heaven, A the hill ， 為君婦 宁处 灰 掃 ， gracious 归 bright 初覆額 4th ；\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import jieba\n",
    "\n",
    "# Read the three text files and store the paragraphs in separate lists\n",
    "with open('output.txt', 'r') as f1, open('长干行.txt', 'r') as f2:\n",
    "    paragraphs1 = f1.read().split('\\n\\n')\n",
    "    paragraphs2 = f2.read().split('\\n\\n')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Split the Chinese text into words\n",
    "words2 = []\n",
    "with open('长干行.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        words = jieba.cut(line.strip(), cut_all=False)\n",
    "        words2.extend(words)\n",
    "\n",
    "# Split the English text into words\n",
    "words1 = []\n",
    "for paragraph in paragraphs1 + paragraphs2:\n",
    "    words = paragraph.split()\n",
    "    words1.extend(words)\n",
    "\n",
    "# Combine the Chinese words and English words in a random order\n",
    "mixed_words = words1 + words2\n",
    "random.shuffle(mixed_words)\n",
    "mixed_text = ' '.join(mixed_words)\n",
    "\n",
    "intertwined = mixed_text\n",
    "print(intertwined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "* Hayes, Brian. “Computer recreations.” Scientific American, vol. 249, no. 5, 1983, pp. 18–31. JSTOR, http://www.jstor.org/stable/24969024. (Original column from Scientific American that described how Markov chain text generation works—very readable! I can send a PDF, hit me up.)\n",
    "* [A Travesty Generator for Micros](https://elmcip.net/critical-writing/travesty-generator-micros) is a follow-up to Hayes' article that has some more theory and an actual Pascal listing (which is now mostly of only historical interest).\n",
    "* [This notebook](https://github.com/aparrish/rwet/blob/master/ngrams-and-markov-chains.ipynb) shows how to implement a Markov chain generator from scratch in Python, if you're interested in such things!\n",
    "* Lillian-Yvonne Bertram's [Travesty Generator](http://www.noemipress.org/catalog/poetry/travesty-generator/) is a striking example of Markov chains put to poetic use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
